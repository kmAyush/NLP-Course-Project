{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dTx56L38_mIR"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import  nltk.translate.bleu_score as bleu\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "utNaFxVi_xZw",
        "outputId": "fcf73873-ac71-4489-bcb7-f211504c84c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6d89f4d8-dd2d-4030-9ef5-1867f2f8b861\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6d89f4d8-dd2d-4030-9ef5-1867f2f8b861\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Hindi_English_Truncated_Corpus.csv to Hindi_English_Truncated_Corpus.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Hindi_English_Truncated_Corpus.csv')\n",
        "df = df.iloc[:,:2]\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qFtJZJIXEo5E",
        "outputId": "d6cb26c5-e8da-49f4-ba22-4b57f45d96fb"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    english_sentence  \\\n",
              "0  politicians do not have permission to do what ...   \n",
              "1         I'd like to tell you about one such child,   \n",
              "2  This percentage is even greater than the perce...   \n",
              "3  what we really mean is that they're bad at not...   \n",
              "4  .The ending portion of these Vedas is called U...   \n",
              "\n",
              "                                      hindi_sentence  \n",
              "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
              "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
              "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
              "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
              "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b2696dc-9ac6-4177-87b3-62fb276e039a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'd like to tell you about one such child,</td>\n",
              "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This percentage is even greater than the perce...</td>\n",
              "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what we really mean is that they're bad at not...</td>\n",
              "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>.The ending portion of these Vedas is called U...</td>\n",
              "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b2696dc-9ac6-4177-87b3-62fb276e039a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9b2696dc-9ac6-4177-87b3-62fb276e039a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9b2696dc-9ac6-4177-87b3-62fb276e039a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-33e9d0bb-8bbf-4787-804c-8e95d863f9c1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-33e9d0bb-8bbf-4787-804c-8e95d863f9c1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-33e9d0bb-8bbf-4787-804c-8e95d863f9c1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [[f\"{row['english_sentence']}\", f\"{row['hindi_sentence']}\"] for index, row in df.iterrows()]\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "hrNoNfkZX8BK"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# Specify the column you want to write to the text file\n",
        "column_name = 'english_sentence'\n",
        "\n",
        "# Open a text file for writing\n",
        "with open('english.txt', 'w') as file:\n",
        "    # Iterate over the values in the specified column\n",
        "    for value in df[column_name]:\n",
        "        # Write each value followed by a newline character\n",
        "        file.write(str(value) + '\\n')\n"
      ],
      "metadata": {
        "id": "T0Kl6B-WGowM"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_name = 'hindi_sentence'\n",
        "\n",
        "# Open a text file for writing\n",
        "with open('hindi.txt', 'w') as file:\n",
        "    # Iterate over the values in the specified column\n",
        "    for value in df[column_name]:\n",
        "        # Write each value followed by a newline character\n",
        "        file.write(str(value) + '\\n')"
      ],
      "metadata": {
        "id": "09Yt_wUjNKY6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the text file for reading\n",
        "with open('english.txt', 'r') as file:\n",
        "    # Read all lines into a list\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Print the contents of the file\n",
        "i = 0\n",
        "for line in lines:\n",
        "    print(line.strip())  # .strip() removes trailing newline characters\n",
        "    i += 1\n",
        "    if i == 5:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbwIwpy_IV7a",
        "outputId": "fc9d2405-a266-4ccb-da71-06a9bf17d03d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "politicians do not have permission to do what needs to be done.\n",
            "I'd like to tell you about one such child,\n",
            "This percentage is even greater than the percentage in India.\n",
            "what we really mean is that they're bad at not paying attention.\n",
            ".The ending portion of these Vedas is called Upanishad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('hindi.txt', 'r') as file:\n",
        "    # Read all lines into a list\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Print the contents of the file\n",
        "i = 0\n",
        "for line in lines:\n",
        "    print(line.strip())  # .strip() removes trailing newline characters\n",
        "    i += 1\n",
        "    if i == 5:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6nld7xdNaUu",
        "outputId": "62d76513-f70c-4f45-832c-41112f7ec659"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .\n",
            "मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,\n",
            "यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।\n",
            "हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते\n",
            "इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty string to store the file contents\n",
        "input_lang = ''\n",
        "\n",
        "# Open the text file for reading\n",
        "with open('english.txt', 'r') as file:\n",
        "    # Read the entire file contents and store them in input_lang\n",
        "    input_lang = file.readlines()\n",
        "\n",
        "# Now, input_lang contains the contents of the file\n",
        "print(len(input_lang))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5TT7K0RZ6EB",
        "outputId": "c1f7b02a-beab-4f3b-db36-c8bdcaefca2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer_eng = Tokenizer(BPE())\n",
        "tokenizer_hin = Tokenizer(BPE())\n",
        "# Initialize a pre-tokenizer\n",
        "tokenizer_eng.pre_tokenizer = Whitespace()\n",
        "tokenizer_hin.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Initialize a trainer\n",
        "trainer = BpeTrainer(special_tokens=[\n",
        "    \"<PAD>\",\n",
        "    \"<SOS>\",\n",
        "    \"<EOS>\",\n",
        "    \"<UNK>\",\n",
        "    \"<BOS>\",\n",
        "\n",
        "])\n",
        "\n",
        "# Training files\n",
        "eng_text = [\"english.txt\"]\n",
        "hin_text = [\"hindi.txt\"]\n",
        "# Train the tokenizer\n",
        "tokenizer_eng.train(eng_text, trainer)\n",
        "tokenizer_hin.train(hin_text, trainer)\n",
        "\n",
        "# Now you can encode text\n",
        "encoded1 = tokenizer_eng.encode(\"Hello, world!\")\n",
        "print(encoded1.ids)\n",
        "print(encoded1.tokens)\n",
        "\n",
        "encoded2 = tokenizer_hin.encode(\"राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है\")\n",
        "print(encoded2.ids)\n",
        "print(encoded2.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7fADLyvINpJ",
        "outputId": "8beba91e-f351-46e9-9c1d-ce5deb0af7cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24804, 16, 521, 5]\n",
            "['Hello', ',', 'world', '!']\n",
            "[16424, 247, 629, 323, 538, 550, 634, 17, 385, 358, 261, 2230, 318, 248]\n",
            "['राजनीतिज्ञों', 'के', 'पास', 'जो', 'कार्य', 'करना', 'चाहिए', ',', 'वह', 'करने', 'कि', 'अनुमति', 'नहीं', 'है']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_vocab = tokenizer_eng.get_vocab()\n",
        "hin_vocab = tokenizer_hin.get_vocab()"
      ],
      "metadata": {
        "id": "zO0nIHfxINsj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [[tokenizer_eng.encode(row['english_sentence']).ids, tokenizer_hin.encode(row['hindi_sentence']).ids] for index, row in df.iterrows()]"
      ],
      "metadata": {
        "id": "d-PLPy3hPpVf"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "pairs_train, pairs_rest = train_test_split(pairs, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the rest into validation and test sets\n",
        "pairs_valid, pairs_test = train_test_split(pairs_rest, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "1Ddr8AB2QHSV"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_data, target_data = self.pairs[idx]\n",
        "\n",
        "        return torch.tensor(input_data), torch.tensor(target_data)\n",
        "\n",
        "\n",
        "# Create an instance of MyDataset\n",
        "\n",
        "# Define a collate function to pad sequences within a batch\n",
        "def pad_collate(batch):\n",
        "    input_seqs, target_seqs = zip(*batch)\n",
        "\n",
        "    # Pad input sequences to the maximum length in the batch\n",
        "    input_seqs_padded = torch.nn.utils.rnn.pad_sequence(input_seqs, batch_first=True)\n",
        "\n",
        "    # Pad target sequences to the maximum length in the batch\n",
        "    target_seqs_padded = torch.nn.utils.rnn.pad_sequence(target_seqs, batch_first=True)\n",
        "\n",
        "    return input_seqs_padded, target_seqs_padded\n",
        "\n",
        "\n",
        "dataset_tr = MyDataset(pairs_train)\n",
        "dataset_val = MyDataset(pairs_valid)\n",
        "dataset_te = MyDataset(pairs_test)\n",
        "# Create a DataLoader\n",
        "batch_size = 5  # Set your desired batch size\n",
        "train_dataloader = DataLoader(dataset_tr, batch_size=batch_size, collate_fn=pad_collate)\n",
        "test_dataloader = DataLoader(dataset_te, batch_size=batch_size, collate_fn=pad_collate)\n",
        "valid_dataloader = DataLoader(dataset_val, batch_size=batch_size, collate_fn=pad_collate)\n",
        "# Example usage of dataloader\n",
        "j = 0\n",
        "for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "    print(\"Batch:\", batch_idx)\n",
        "    print(\"Input shape:\", inputs.shape)\n",
        "    print(\"Target shape:\", targets)\n",
        "    j += 1\n",
        "    if j == 1:\n",
        "      break"
      ],
      "metadata": {
        "id": "833ugnM4QHVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size,embedding_size, hidden_size,p, layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layers = layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, layers, batch_first=False, dropout = p, bidirectional = True)\n",
        "\n",
        "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        print(type(self.dropout),type(self.embedding))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        print(\"input encoder \",x.shape)\n",
        "        print(\"input encoder \",x)\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        print(\"encoder embed shape \", embedding.shape)\n",
        "        encoder_states, (hidden, cell) = self.lstm(embedding)\n",
        "        print(\"encoder state in encoder \", encoder_states.shape)\n",
        "        print(\"hidden in encoder \",hidden.shape)\n",
        "        print(\"cell in encoder \", cell.shape)\n",
        "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
        "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "        print(\"cat hidden in encoder \",hidden.shape)\n",
        "        print(\"cat cell in encoder \", cell.shape)\n",
        "        return encoder_states, hidden, cell\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "DPNkPQhC2Dth"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self,input_size, embedding_size, hidden_size, p, layers = 1 ):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(hidden_size*2+embedding_size , hidden_size,layers, dropout = p)\n",
        "\n",
        "        self.energy = nn.Linear(hidden_size * 3, 1)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.relu = nn.ReLU()\n",
        "        print(type(self.dropout),type(self.embedding))\n",
        "\n",
        "    def forward(self, x, encoder_states, hidden, cell):\n",
        "        ## CODE CHANGED\n",
        "        print(\"x value in decoder \",x)\n",
        "        print(\"encoder state \",encoder_states.shape)\n",
        "\n",
        "        # print(x,encoder_states.shape,hidden.shape,cell.shape)\n",
        "        ## CODE CHANGED\n",
        "        x = x.unsqueeze(0)\n",
        "        # x -> (1, N)\n",
        "        print(\"size of x in decoder \",x.shape)\n",
        "        embeddings = self.dropout(self.embedding(x))\n",
        "        # embeddings -> (1, N, embed_size)\n",
        "        print(\"embeddings \",embeddings.shape)\n",
        "        sequence_length = encoder_states.shape[0]\n",
        "        print(\"sequence_length \",sequence_length)\n",
        "        h_reshaped = hidden.repeat(sequence_length,1,1)\n",
        "        # h_reshape -> (seq_length, N, hidden )\n",
        "        print(\"h_rshape \",h_reshaped.shape)\n",
        "\n",
        "        energy = self.relu(self.energy(torch.cat((h_reshaped,encoder_states), dim =2)))\n",
        "        # energy = self.relu(self.energy(torch.cat((h_reshaped,encoder_states), dim =2)))\n",
        "        # energy -> (seq_length, N, 1)\n",
        "        print(\"print shape \", energy.shape)\n",
        "        attention = self.softmax(energy)\n",
        "        # attention = self.softmax(energy.transpose(0, 1))\n",
        "        # attension -> (seq_length, N, 1)\n",
        "        print(\"attension shape \", attention.shape)\n",
        "        weighted_states = attention  * encoder_states\n",
        "        context_vector = torch.sum(weighted_states, dim=1)\n",
        "\n",
        "        print(\"shape of weight vector \",weighted_states.shape)\n",
        "        # context_vector = torch.matmul(attention.transpose(1, 2), encoder_states)\n",
        "        # context_vector = context_vector.squeeze(1)\n",
        "        embeddings = embeddings.expand(context_vector.shape[0], -1)\n",
        "\n",
        "        print(\"context vector shape \", context_vector.shape)\n",
        "        # rnn_input = torch.cat((context_vector, embeddings), dim=2)\n",
        "        rnn_input = torch.cat((context_vector, embeddings), dim=1)\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        predictions = self.fc(outputs).squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell"
      ],
      "metadata": {
        "id": "KpPdfCvX2Dw0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self,encoder, decoder):\n",
        "        super(Seq2seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio = 0.7):\n",
        "        batch_size = source.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        # target = target.transpose(0, 1)\n",
        "        target_vocab_size = len(hin_vocab)\n",
        "        print(\"source shape \",source.shape)\n",
        "        print(\"source target \",target.shape)\n",
        "        print(\"target length \", target_len)\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "        x = target[0]\n",
        "        print(\"x in seq2seq \", x)\n",
        "        print(\"x in seq2seq \", x.shape)\n",
        "        for t in range(0, target_len):\n",
        "            # At every time step use encoder_states and update hidden, cell\n",
        "            output, hidden, cell = self.decoder(x[t], encoder_states, hidden, cell)\n",
        "\n",
        "            # Store prediction for current time step\n",
        "            outputs[t] = output\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "VxZWVL552DyZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "metadata": {
        "id": "Kh_ak0ojhKkS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNTrainer:\n",
        "    \"\"\"\n",
        "    RNNTrainer wraps RNN_LM to handle training and evaluation.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        train_dataloader,\n",
        "        valid_dataloader,\n",
        "        epochs,\n",
        "        use_cuda,\n",
        "        eng_vocab,\n",
        "        # hin_vocab,\n",
        "        model_dir\n",
        "    ):\n",
        "\n",
        "        self.model = model,\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.valid_dataloader = valid_dataloader\n",
        "        self.use_cuda = use_cuda\n",
        "        self.model_dir = model_dir\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.eng_vocab = eng_vocab\n",
        "        self.hin_vocab = hin_vocab\n",
        "\n",
        "\n",
        "        # Move the model to GPU if available\n",
        "        if self.use_cuda:\n",
        "            self.model = self.model.cuda()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains the model with train_dataloader and validates using valid_dataloader\n",
        "\n",
        "        \"\"\"\n",
        "        # You may change the input arguments to this function,\n",
        "        # but make sure to also change the code wherever this function is called\n",
        "\n",
        "        # ADD YOUR CODE HERE FOR TRAINING & VALIDATION\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        patience = 10\n",
        "        no_impr_counter = 0\n",
        "\n",
        "\n",
        "        for epoch in tqdm(range(self.epochs)):\n",
        "          loss_train = self.train_model()\n",
        "          loss_val = self.test_model()\n",
        "\n",
        "          print(f\"epoch number : {epoch+1}, training loss : {loss_train}, validation loss : {loss_val}\")\n",
        "\n",
        "          if loss_val < best_val_loss:\n",
        "                best_val_loss = loss_val\n",
        "                no_impr_counter = 0\n",
        "                self.save_model()\n",
        "          else:\n",
        "            no_impr_counter += 1\n",
        "\n",
        "          if(no_impr_counter>patience):\n",
        "               print(f\"early stopping at epoch {epoch+1} since no improvement from last {patience} epoch\")\n",
        "               break;\n",
        "\n",
        "\n",
        "    def train_model(self):\n",
        "          epoch_loss = 0\n",
        "          j = 0\n",
        "          self.model = model\n",
        "          model.train()\n",
        "\n",
        "          for input, target in self.train_dataloader:\n",
        "              if self.use_cuda:\n",
        "                    input, target = input.cuda(), target.cuda()\n",
        "\n",
        "              print(\"input_shape in seq2seq \",input.shape)\n",
        "              output = self.model(input, target)\n",
        "\n",
        "              output = output[1:].reshape(-1, output.shape[2])\n",
        "              target = target[1:].reshape(-1)\n",
        "\n",
        "              self.optimizer.zero_grad()\n",
        "              loss = self.criterion(output, target)\n",
        "\n",
        "              # print(loss_value)\n",
        "              loss.backward()\n",
        "              loss_value = loss.item()\n",
        "              torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1)\n",
        "\n",
        "              self.optimizer.step()\n",
        "              epoch_loss += loss_value\n",
        "              j+=1\n",
        "\n",
        "          self.loss[\"train\"].append(epoch_loss/j)\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "          return epoch_loss/j\n",
        "\n",
        "    def test_model(self ):\n",
        "          epoch_loss = 0\n",
        "          j = 0\n",
        "          self.model_seq2seq.eval()\n",
        "          # batch_size = targets.size(0)\n",
        "\n",
        "          for input, target in (self.valid_dataloader):\n",
        "              if self.use_cuda:\n",
        "                    input, target = input.cuda(), target.cuda()\n",
        "\n",
        "              output = self.model_seq2seq(input)\n",
        "              output = output[1:].reshape(-1, output.shape[2])\n",
        "              target = target[1:].reshape(-1)\n",
        "\n",
        "              loss = self.criterion(output, target)\n",
        "              loss_value = loss.item()\n",
        "              epoch_loss += loss_value\n",
        "              j+=1\n",
        "\n",
        "          self.loss[\"val\"].append(epoch_loss/j)\n",
        "          torch.cuda.empty_cache()\n",
        "          return epoch_loss/j\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save final model to directory\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "        print(\"=> Saving checkpoint\")\n",
        "        torch.save(state, filename)\n",
        "\n",
        "\n",
        "    def bleu(data, model, german, english, device):\n",
        "       targets = []\n",
        "       outputs = []\n",
        "\n",
        "       for example in data:\n",
        "         src = vars(example)[\"src\"]\n",
        "         trg = vars(example)[\"trg\"]\n",
        "\n",
        "         prediction = translate_sentence(model, src, german, english, device)\n",
        "         prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "         targets.append([trg])\n",
        "         outputs.append(prediction)\n",
        "\n",
        "       return bleu_score(outputs, targets)\n",
        "\n",
        "    def save_loss(self):\n",
        "        \"\"\"\n",
        "        Save train/val loss as json file to the directory\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
        "        with open(loss_path, \"w\") as fp:\n",
        "            json.dump(self.loss, fp)"
      ],
      "metadata": {
        "id": "rX0Bdq34XAB6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "# Remember to fix seed as 42\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# check if GPU is available\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "print(f\"GPU is available: {USE_CUDA}\")\n",
        "\n",
        "batch_size = 32\n",
        "SHUFFLE = True# if dataset should be shuffled\n",
        "\n"
      ],
      "metadata": {
        "id": "5spM_ozB2D1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c21285-52a2-4b31-de9c-22a87acb585a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "input_size_encoder = len(eng_vocab)\n",
        "encoder_embedding_size = 200\n",
        "hidden_size = 200\n",
        "num_layers = 1\n",
        "enc_dropout = 0.3\n",
        "\n",
        "input_size_decoder = len(hin_vocab)\n",
        "decoder_embedding_size = 250\n",
        "dec_dropout = 0.3\n",
        "\n",
        "encoder_net = EncoderRNN(input_size_encoder, encoder_embedding_size, hidden_size, enc_dropout, num_layers).to(device)\n",
        "decoder_net = DecoderRNN( input_size_decoder, decoder_embedding_size, hidden_size, dec_dropout, num_layers).to(device)\n",
        "\n",
        "model = Seq2seq(encoder_net, decoder_net).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2ID0h3FS3w2",
        "outputId": "0712251b-7dcb-446c-c5f2-4af952e43753"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.nn.modules.dropout.Dropout'> <class 'torch.nn.modules.sparse.Embedding'>\n",
            "<class 'torch.nn.modules.dropout.Dropout'> <class 'torch.nn.modules.sparse.Embedding'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "EPOCHS = 20\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "CRTbXzqLVO7l"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = 'suprit_22838/rnn'\n",
        "\n",
        "# if not os.path.exists(model_dir):\n",
        "#     os.makedirs(model_dir)\n",
        "\n",
        "# NOTE: if you are **optionally** using additional options for the trainer\n",
        "# (e.g., a training scheduler), please add them below.\n",
        "trainer = RNNTrainer(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion,\n",
        "        train_dataloader=train_dataloader,\n",
        "        valid_dataloader=valid_dataloader,\n",
        "        epochs=EPOCHS,\n",
        "        use_cuda=USE_CUDA,\n",
        "        eng_vocab = eng_vocab,\n",
        "        model_dir=model_dir\n",
        "        )\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "trainer.save_model()\n",
        "trainer.save_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mBRjiR4PzvGG",
        "outputId": "6883dd76-e1c5-4ecb-af3f-e80f83775689"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_shape in seq2seq  torch.Size([5, 37])\n",
            "source shape  torch.Size([5, 37])\n",
            "source target  torch.Size([5, 49])\n",
            "target length  49\n",
            "input encoder  torch.Size([5, 37])\n",
            "input encoder  tensor([[  980,   741,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0],\n",
            "        [  943,  1786,   524,   203,  6672,  3168,   234,   577,    16, 14961,\n",
            "           245,   285, 29473, 29612,    16,   203,  3257, 18690,   200,   245,\n",
            "          2848,   324,   273,   186,  2834,    16,   602,   200,  2985,   204,\n",
            "          6098,  5038,   194,   599,   200,  2407,    18],\n",
            "        [  185,  1439,   186,  2164,   194, 16763,   233,   496,    17,   540,\n",
            "           191,  1289,   302,  2323,   204,  2407,   273, 10996,    80,   839,\n",
            "          1751,   200,  1278,   200,  1099,   186,  2407,    18,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0],\n",
            "        [   45,   245,  2503,  2857,   194,   503,   186,  4385,  1397,   772,\n",
            "            18,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 1306,   628,   208,  4781,   192,   299,   225,    69,  1078,  8767,\n",
            "            16,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0]])\n",
            "encoder embed shape  torch.Size([5, 37, 200])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder state in encoder  torch.Size([5, 37, 400])\n",
            "hidden in encoder  torch.Size([2, 37, 200])\n",
            "cell in encoder  torch.Size([2, 37, 200])\n",
            "cat hidden in encoder  torch.Size([1, 37, 200])\n",
            "cat cell in encoder  torch.Size([1, 37, 200])\n",
            "x in seq2seq  tensor([930,  31, 867,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0])\n",
            "x in seq2seq  torch.Size([49])\n",
            "x value in decoder  tensor(930)\n",
            "encoder state  torch.Size([5, 37, 400])\n",
            "size of x in decoder  torch.Size([1])\n",
            "embeddings  torch.Size([1, 250])\n",
            "sequence_length  5\n",
            "h_rshape  torch.Size([5, 37, 200])\n",
            "print shape  torch.Size([5, 37, 1])\n",
            "attension shape  torch.Size([5, 37, 1])\n",
            "shape of weight vector  torch.Size([5, 37, 400])\n",
            "context vector shape  torch.Size([5, 400])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-5e5c7d1422ae>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bb7a487d11c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m           \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bb7a487d11c0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_shape in seq2seq \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m               \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m               \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-8b956ef41389>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# At every time step use encoder_states and update hidden, cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Store prediction for current time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-bf565b362323>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_states, hidden, cell)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# outputs shape: (1, N, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    868\u001b[0m                         msg = (\"For unbatched 2-D input, hx and cx should \"\n\u001b[1;32m    869\u001b[0m                                f\"also be 2-D but got ({hx[0].dim()}-D, {hx[1].dim()}-D) tensors\")\n\u001b[0;32m--> 870\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m                     \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;31m# Each batch of the hidden state should match the input sequence that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder1, attn_decoder1 = run_training()"
      ],
      "metadata": {
        "id": "hOBnWgTY5zZL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71a50081-6c38-4377-9351-e8dc1a545908"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Building Encoder... ==========\n",
            "\n",
            "EncoderRNN(\n",
            "  (embedding): Embedding(66550, 100, padding_idx=1)\n",
            "  (lstm): LSTM(100, 100)\n",
            ")\n",
            "\n",
            "\n",
            "+-------------------+------------+\n",
            "|      Modules      | Parameters |\n",
            "+-------------------+------------+\n",
            "|  embedding.weight |  6655000   |\n",
            "| lstm.weight_ih_l0 |   40000    |\n",
            "| lstm.weight_hh_l0 |   40000    |\n",
            "|  lstm.bias_ih_l0  |    400     |\n",
            "|  lstm.bias_hh_l0  |    400     |\n",
            "+-------------------+------------+\n",
            "Total Trainable Params: 6735800\n",
            "\n",
            "\n",
            "========== Building Decoder... ==========\n",
            "\n",
            "AttnDecoderRNN(\n",
            "  (embedding): Embedding(49371, 100)\n",
            "  (attn): Linear(in_features=200, out_features=20, bias=True)\n",
            "  (attn_combine): Linear(in_features=200, out_features=100, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (lstm): LSTM(100, 100)\n",
            "  (out): Linear(in_features=100, out_features=49371, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "+---------------------+------------+\n",
            "|       Modules       | Parameters |\n",
            "+---------------------+------------+\n",
            "|   embedding.weight  |  4937100   |\n",
            "|     attn.weight     |    4000    |\n",
            "|      attn.bias      |     20     |\n",
            "| attn_combine.weight |   20000    |\n",
            "|  attn_combine.bias  |    100     |\n",
            "|  lstm.weight_ih_l0  |   40000    |\n",
            "|  lstm.weight_hh_l0  |   40000    |\n",
            "|   lstm.bias_ih_l0   |    400     |\n",
            "|   lstm.bias_hh_l0   |    400     |\n",
            "|      out.weight     |  4937100   |\n",
            "|       out.bias      |   49371    |\n",
            "+---------------------+------------+\n",
            "Total Trainable Params: 10028491\n",
            "\n",
            "Total parameters in encoder + decoder :  16764291\n",
            "\n",
            "\n",
            "========== Starting Training... ==========\n",
            "\n",
            "0m 0s (- 0m 2s) (10 10%) 10.7829\n",
            "0m 0s (- 0m 1s) (20 20%) 5.5414\n",
            "0m 0s (- 0m 1s) (30 30%) 3.1057\n",
            "0m 0s (- 0m 1s) (40 40%) 6.8275\n",
            "0m 0s (- 0m 0s) (50 50%) 8.0003\n",
            "0m 1s (- 0m 0s) (60 60%) 4.8760\n",
            "0m 1s (- 0m 0s) (70 70%) 3.4653\n",
            "0m 1s (- 0m 0s) (80 80%) 2.9124\n",
            "0m 1s (- 0m 0s) (90 90%) 4.3261\n",
            "0m 1s (- 0m 0s) (100 100%) 5.9385\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Finished Training... ==========\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "metadata": {
        "id": "iIpowBGP5zbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0c2be6-ca46-4571-ef1e-e8ff95807eb0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The average age of a circus elephant is about 35 years .\n",
            "= सर्कस के हाथी की औसत उम्र 35 वर्ष होती है .\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> so I wrote about this and had it translated\n",
            "= इसलिए मैने इसके बारे में लिखा, और इसका चीनी में अनुवाद करवाया\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> But Hitler is not so mere interested in subject of India.\n",
            "= लेकिन हिटलर को भारत के विषय में विशेष रूची नहीं थी।\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> all those problems.\n",
            "= इस तरह की समस्याएँ |\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> We stop for a quick commercial break,\n",
            "= हम एक छोटे से कमर्शियल ब्रेक के लिए रुकेंगे,\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> So, I'll start with this:\n",
            "= तो, मैं इससे शुरूआत करूँगी :\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> On top of this building there is a big tomb\n",
            "= इस इमारत के ऊपर एक वृहत गुम्बद सुशोभित है।\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> Karna : son of Kunti and Suryadev and eldest brother of Pandavas.\n",
            "= कर्ण : सूर्यदेव एवमं कुन्ती के पुत्र और पाण्डवों के सबसे बड़े भाई।\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> of shame and fear\n",
            "= शर्म और डर के\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n",
            "> in remembrance of him there is a big statue here\n",
            "= उनकी याद में एक बड़ि मूर्ति यहां स्थापित है।\n",
            "< <EOS>\n",
            "BLEU Score 0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}