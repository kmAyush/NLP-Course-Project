# Video Dubbing with ML-driven Emotional Expression and Lip Synchronization
Dubbing videos from one language to another using machine learning (ML)
models presents a myriad of challenges, ranging from achieving precise
lip synchronization to conveying emotional expression and preserving
natural pauses between sentences. While current state-of-the-art models
make strides in addressing certain facets of these challenges, they often fall
short of delivering optimal results. For instance, the Hierarchical Prosody
Model adeptly handles dubbing based on translated text but struggles with
lip synchronization nuances. Conversely, models like Wav2Lip excel in
ensuring lip synchronization but face obstacles due to output dependency
issues arising from the requirement for translated speech.

## Team Members
Ayush Krishna Murthi / Suprit Chafle / Akhil
